{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "from multiprocessing import Process\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from yolov5.train_dt import *\n",
    "from EfficientObjectDetection.train_800 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "# # replace the classifier with a new one, that has\n",
    "# # num_classes which is user-defined\n",
    "# num_classes = 1  # 1 class (person) + background\n",
    "# # get number of input features for the classifier\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# # replace the pre-trained head with a new one\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_opt_tr = easydict.EasyDict({\n",
    "    \"cfg\": \"/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/models/yolov5x_custom.yaml\",\n",
    "    \"data\": \"/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/data/HRSID_800_od.yaml\",\n",
    "    \"hyp\": '',\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 48,\n",
    "    \"img_size\": [480, 480],\n",
    "    \"rect\": False,\n",
    "    \"resume\": False,\n",
    "    \"nosave\": True,\n",
    "    \"notest\": True,\n",
    "    \"noautoanchor\": True,\n",
    "    \"evolve\": False,\n",
    "    \"bucket\": '',\n",
    "    \"cache_images\": False,\n",
    "    \"weights\": \" \",\n",
    "    \"name\": \"yolov5x_800_480_10epoch\",\n",
    "    \"device\": \"0, 1, 2\",\n",
    "    \"multi_scale\": False,\n",
    "    \"single_cls\": True,\n",
    "    \"sync_bn\": False,\n",
    "    \"local_rank\": -1\n",
    "})\n",
    "\n",
    "fine_opt_eval = easydict.EasyDict({\n",
    "    \"data\": \"/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/data/HRSID_800_rl.yaml\",\n",
    "    \"batch_size\": 1,\n",
    "    \"conf_thres\": 0.001,\n",
    "    \"iou_thres\": 0.6  # for NMS\n",
    "})\n",
    "\n",
    "coarse_opt_tr = easydict.EasyDict({\n",
    "    \"cfg\": \"/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/models/yolov5x_custom.yaml\",\n",
    "    \"data\": \"/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/data/HRSID_800_od.yaml\",\n",
    "    \"hyp\": '',\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 48,\n",
    "    \"img_size\": [96, 96],\n",
    "    \"rect\": False,\n",
    "    \"resume\": False,\n",
    "    \"nosave\": True,\n",
    "    \"notest\": True,\n",
    "    \"noautoanchor\": True,\n",
    "    \"evolve\": False,\n",
    "    \"bucket\": '',\n",
    "    \"cache_images\": False,\n",
    "    \"weights\": \" \",\n",
    "    \"name\": \"yolov5x_800_96_10epoch\",\n",
    "    \"device\": \"0, 1, 2\",\n",
    "    \"multi_scale\": False,\n",
    "    \"single_cls\": True,\n",
    "    \"sync_bn\": False,\n",
    "    \"local_rank\": -1\n",
    "})\n",
    "\n",
    "coarse_opt_eval = easydict.EasyDict({\n",
    "    \"data\": \"/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/data/HRSID_800_rl.yaml\",\n",
    "    \"batch_size\": 1,\n",
    "    \"conf_thres\": 0.001,\n",
    "    \"iou_thres\": 0.6  # for NMS\n",
    "})\n",
    "\n",
    "EfficientOD_opt = easydict.EasyDict({\n",
    "    \"gpu_id\": \"0, 1, 2\",\n",
    "    \"lr\": 1e-3,\n",
    "    \"load\": None,\n",
    "    \"cv_dir\": \"cv/tmp/\",\n",
    "    \"batch_size\": 1,\n",
    "    \"img_size\": 480,\n",
    "    \"epoch_step\": 10000,\n",
    "    \"max_epochs\": 300,\n",
    "    \"num_workers\": 0,\n",
    "    \"test_epoch\": 5,\n",
    "    \"parallel\": True,\n",
    "    \"alpha\": 0.8,\n",
    "    \"beta\": 0.1,\n",
    "    \"sigma\": 0.5\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device for EfficientOD:  True\n"
     ]
    }
   ],
   "source": [
    "fine_detector = yolov5(fine_opt_tr, fine_opt_eval)\n",
    "coarse_detector = yolov5(coarse_opt_tr, coarse_opt_eval)\n",
    "rl_agent = EfficientOD(EfficientOD_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32480MB)\n",
      "           device1 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32480MB)\n",
      "           device2 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32480MB)\n",
      "\n",
      "{'cfg': '/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/models/yolov5x_custom.yaml', 'data': '/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/data/HRSID_800_od.yaml', 'hyp': '', 'epochs': 100, 'batch_size': 48, 'img_size': [480, 480], 'rect': False, 'resume': False, 'nosave': True, 'notest': True, 'noautoanchor': True, 'evolve': False, 'bucket': '', 'cache_images': False, 'weights': ' ', 'name': 'yolov5x_800_480_10epoch', 'device': '0, 1, 2', 'multi_scale': False, 'single_cls': True, 'sync_bn': False, 'local_rank': -1, 'total_batch_size': 48, 'world_size': 1}\n",
      "Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n",
      "Hyperparameters {'optimizer': 'SGD', 'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      8800  yolov5.models.common.Focus              [3, 80, 3]                    \n",
      "  1                -1  1    115520  yolov5.models.common.Conv               [80, 160, 3, 2]               \n",
      "  2                -1  1    315680  yolov5.models.common.BottleneckCSP      [160, 160, 4]                 \n",
      "  3                -1  1    461440  yolov5.models.common.Conv               [160, 320, 3, 2]              \n",
      "  4                -1  1   3311680  yolov5.models.common.BottleneckCSP      [320, 320, 12]                \n",
      "  5                -1  1   1844480  yolov5.models.common.Conv               [320, 640, 3, 2]              \n",
      "  6                -1  1  13228160  yolov5.models.common.BottleneckCSP      [640, 640, 12]                \n",
      "  7                -1  1   7375360  yolov5.models.common.Conv               [640, 1280, 3, 2]             \n",
      "  8                -1  1   4099840  yolov5.models.common.SPP                [1280, 1280, [5, 9, 13]]      \n",
      "  9                -1  1  20087040  yolov5.models.common.BottleneckCSP      [1280, 1280, 4, False]        \n",
      " 10                -1  1    820480  yolov5.models.common.Conv               [1280, 640, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 13                -1  1   5435520  yolov5.models.common.BottleneckCSP      [1280, 640, 4, False]         \n",
      " 14                -1  1    205440  yolov5.models.common.Conv               [640, 320, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 17                -1  1   1360960  yolov5.models.common.BottleneckCSP      [640, 320, 4, False]          \n",
      " 18                -1  1    922240  yolov5.models.common.Conv               [320, 320, 3, 2]              \n",
      " 19          [-1, 14]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 20                -1  1   5025920  yolov5.models.common.BottleneckCSP      [640, 640, 4, False]          \n",
      " 21                -1  1   3687680  yolov5.models.common.Conv               [640, 640, 3, 2]              \n",
      " 22          [-1, 10]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 23                -1  1  20087040  yolov5.models.common.BottleneckCSP      [1280, 1280, 4, False]        \n",
      " 24      [17, 20, 23]  1     40374  yolov5.models.yolo.Detect               [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
      "Model Summary: 407 layers, 8.84337e+07 parameters, 8.84337e+07 gradients\n",
      "\n",
      "Optimizer groups: 134 .bias, 142 conv.weight, 131 other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning labels /home/SSDD/ICIP21_dataset/split_data_4_0/dt_ver/train/labels.cache (5299 found, 0 missing, 0 empty, 0 duplicate, for 5299 images): 100%|██████████| 5299/5299 [00:00<00:00, 15354.24it/s]\n",
      "Scanning labels /home/SSDD/ICIP21_dataset/split_data_4_0/dt_ver/val/labels.cache (1305 found, 0 missing, 0 empty, 0 duplicate, for 1305 images): 100%|██████████| 1305/1305 [00:00<00:00, 15215.47it/s]\n",
      "Scanning labels /home/SSDD/ICIP21_dataset/split_data_4_0/dt_ver/val/labels.cache (1305 found, 0 missing, 0 empty, 0 duplicate, for 1305 images): 100%|██████████| 1305/1305 [00:00<00:00, 14947.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32480MB)\n",
      "           device1 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32480MB)\n",
      "           device2 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32480MB)\n",
      "\n",
      "{'cfg': '/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/models/yolov5x_custom.yaml', 'data': '/home/cutz/SAR/SAR_OD/EfficientOD_WACV/yolov5/data/HRSID_800_od.yaml', 'hyp': '', 'epochs': 100, 'batch_size': 48, 'img_size': [96, 96], 'rect': False, 'resume': False, 'nosave': True, 'notest': True, 'noautoanchor': True, 'evolve': False, 'bucket': '', 'cache_images': False, 'weights': ' ', 'name': 'yolov5x_800_96_10epoch', 'device': '0, 1, 2', 'multi_scale': False, 'single_cls': True, 'sync_bn': False, 'local_rank': -1, 'total_batch_size': 48, 'world_size': 1}\n",
      "Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n",
      "Hyperparameters {'optimizer': 'SGD', 'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      8800  yolov5.models.common.Focus              [3, 80, 3]                    \n",
      "  1                -1  1    115520  yolov5.models.common.Conv               [80, 160, 3, 2]               \n",
      "  2                -1  1    315680  yolov5.models.common.BottleneckCSP      [160, 160, 4]                 \n",
      "  3                -1  1    461440  yolov5.models.common.Conv               [160, 320, 3, 2]              \n",
      "  4                -1  1   3311680  yolov5.models.common.BottleneckCSP      [320, 320, 12]                \n",
      "  5                -1  1   1844480  yolov5.models.common.Conv               [320, 640, 3, 2]              \n",
      "  6                -1  1  13228160  yolov5.models.common.BottleneckCSP      [640, 640, 12]                \n",
      "  7                -1  1   7375360  yolov5.models.common.Conv               [640, 1280, 3, 2]             \n",
      "  8                -1  1   4099840  yolov5.models.common.SPP                [1280, 1280, [5, 9, 13]]      \n",
      "  9                -1  1  20087040  yolov5.models.common.BottleneckCSP      [1280, 1280, 4, False]        \n",
      " 10                -1  1    820480  yolov5.models.common.Conv               [1280, 640, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 13                -1  1   5435520  yolov5.models.common.BottleneckCSP      [1280, 640, 4, False]         \n",
      " 14                -1  1    205440  yolov5.models.common.Conv               [640, 320, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 17                -1  1   1360960  yolov5.models.common.BottleneckCSP      [640, 320, 4, False]          \n",
      " 18                -1  1    922240  yolov5.models.common.Conv               [320, 320, 3, 2]              \n",
      " 19          [-1, 14]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 20                -1  1   5025920  yolov5.models.common.BottleneckCSP      [640, 640, 4, False]          \n",
      " 21                -1  1   3687680  yolov5.models.common.Conv               [640, 640, 3, 2]              \n",
      " 22          [-1, 10]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 23                -1  1  20087040  yolov5.models.common.BottleneckCSP      [1280, 1280, 4, False]        \n",
      " 24      [17, 20, 23]  1     40374  yolov5.models.yolo.Detect               [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
      "Model Summary: 407 layers, 8.84337e+07 parameters, 8.84337e+07 gradients\n",
      "\n",
      "Optimizer groups: 134 .bias, 142 conv.weight, 131 other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning labels /home/SSDD/ICIP21_dataset/split_data_4_0/dt_ver/train/labels.cache (5299 found, 0 missing, 0 empty, 0 duplicate, for 5299 images): 100%|██████████| 5299/5299 [00:00<00:00, 17033.72it/s]\n",
      "Scanning labels /home/SSDD/ICIP21_dataset/split_data_4_0/dt_ver/val/labels.cache (1305 found, 0 missing, 0 empty, 0 duplicate, for 1305 images): 100%|██████████| 1305/1305 [00:00<00:00, 17421.20it/s]\n",
      "Scanning labels /home/SSDD/ICIP21_dataset/split_data_4_0/dt_ver/val/labels.cache (1305 found, 0 missing, 0 empty, 0 duplicate, for 1305 images): 100%|██████████| 1305/1305 [00:00<00:00, 17293.89it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "fine_detector.main(epochs)\n",
    "coarse_detector.main(epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f12819b9590>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(epoch, det, fr_model):\n",
    "    t0 = time.time()\n",
    "    rank = det.opt.local_rank\n",
    "    if rank in [0, -1]:\n",
    "        print('Image sizes %g train, %g test' % (det.imgsz, det.imgsz_test))\n",
    "        print('Using %g dataloader workers' % det.dataloader.num_workers)\n",
    "        print('Starting training for %g epochs...' % det.epochs)\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    ##########################################################################################################\n",
    "    # Start training\n",
    "    for epoch in range(e, e + 1):  # epoch ------------------------------------------------------------------\n",
    "        det.model.train()\n",
    "\n",
    "        # Update image weights (optional)\n",
    "        # When in DDP mode, the generated indices will be broadcasted to synchronize dataset.\n",
    "        if det.dataset.image_weights:\n",
    "            # Generate indices.\n",
    "            if rank in [-1, 0]:\n",
    "                w = det.model.class_weights.cpu().numpy() * (1 - det.maps) ** 2  # class weights\n",
    "                image_weights = labels_to_image_weights(det.dataset.labels, nc=det.nc, class_weights=w)\n",
    "                det.dataset.indices = random.choices(range(det.dataset.n), weights=image_weights,\n",
    "                                                 k=det.dataset.n)  # rand weighted idx\n",
    "            # Broadcast.\n",
    "            if rank != -1:\n",
    "                indices = torch.zeros([det.dataset.n], dtype=torch.int)\n",
    "                if rank == 0:\n",
    "                    indices[:] = torch.from_tensor(det.dataset.indices, dtype=torch.int)\n",
    "                dist.broadcast(indices, 0)\n",
    "                if rank != 0:\n",
    "                    det.dataset.indices = indices.cpu().numpy()\n",
    "\n",
    "        # Update mosaic border\n",
    "        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
    "        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
    "\n",
    "        det.mloss = torch.zeros(4, device=det.device)  # mean losses\n",
    "        if rank != -1:\n",
    "            det.dataloader.sampler.set_epoch(epoch)\n",
    "        pbar = enumerate(det.dataloader)\n",
    "        if rank in [-1, 0]:\n",
    "            print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n",
    "            pbar = tqdm(pbar, total=det.nb)  # progress bar\n",
    "        det.optimizer.zero_grad()\n",
    "\n",
    "        # batch -------------------------------------------------------------\n",
    "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "            ni = i + det.nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = imgs.to(det.device, non_blocking=True).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n",
    "\n",
    "            # Warmup\n",
    "            if ni <= det.nw:\n",
    "                xi = [0, det.nw]  # x interp\n",
    "                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n",
    "                self.accumulate = max(1, np.interp(ni, xi, [1, det.nbs / det.total_batch_size]).round())\n",
    "                for j, x in enumerate(det.optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, xi, [0.1 if j == 2 else 0.0, x['initial_lr'] * det.lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, xi, [0.9, det.hyp['momentum']])\n",
    "\n",
    "            # Multi-scale\n",
    "            if det.opt.multi_scale:\n",
    "                sz = random.randrange(det.imgsz * 0.5, det.imgsz * 1.5 + det.gs) // det.gs * det.gs  # size\n",
    "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "                if sf != 1:\n",
    "                    ns = [math.ceil(x * sf / det.gs) * det.gs for x in\n",
    "                          imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Faster-RCNN Forward\n",
    "            fr_model.train()\n",
    "            loss_dict = fr_model(imgs, targets) # batch\n",
    "            \n",
    "            # Loss\n",
    "\n",
    "#             # Loss\n",
    "#             loss, loss_items = compute_loss(pred, targets.to(self.device), self.model)  # scaled by batch_size\n",
    "#             if rank != -1:\n",
    "#                 loss *= self.opt.world_size  # gradient averaged between devices in DDP mode\n",
    "#             if not torch.isfinite(loss):\n",
    "#                 print('WARNING: non-finite loss, ending training ', loss_items)\n",
    "#                 return self.results\n",
    "\n",
    "            # Backward\n",
    "            if mixed_precision:\n",
    "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # Optimize\n",
    "            if ni % self.accumulate == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                if self.ema is not None:\n",
    "                    self.ema.update(self.model)\n",
    "\n",
    "            # Print\n",
    "            if rank in [-1, 0]:\n",
    "                self.mloss = (self.mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "                mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n",
    "                self.s = ('%10s' * 2 + '%10.4g' * 6) % (\n",
    "                    '%g/%g' % (epoch, self.epochs - 1), mem, * self.mloss, targets.shape[0], imgs.shape[-1])\n",
    "                pbar.set_description(self.s)\n",
    "\n",
    "                # Plot\n",
    "                if ni < 3:\n",
    "                    f = str(Path(self.log_dir) / ('train_batch%g.jpg' % ni))  # filename\n",
    "                    result = plot_images(images=imgs, targets=targets, paths=paths, fname=f)\n",
    "                    if self.tb_writer and result is not None:\n",
    "                        self.tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)\n",
    "                        # tb_writer.add_graph(model, imgs)  # add model to tensorboard\n",
    "\n",
    "            # end batch --------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Scheduler\n",
    "        self.scheduler.step()\n",
    "\n",
    "        # Only the first process in DDP mode is allowed to log or save checkpoints.\n",
    "        if rank in [-1, 0]:\n",
    "            # mAP\n",
    "            if self.ema is not None:\n",
    "                self.ema.update_attr(self.model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride'])\n",
    "            # final_epoch = epoch + 1 == self.epochs\n",
    "            final_epoch = True\n",
    "            if not self.opt.notest or final_epoch:  # Calculate mAP\n",
    "                self.results, maps, times = test.test(self.opt.data,\n",
    "                                                 batch_size=self.total_batch_size,\n",
    "                                                 imgsz=self.imgsz_test,\n",
    "                                                 save_json=final_epoch and self.opt.data.endswith(\n",
    "                                                     os.sep + 'coco.yaml'),\n",
    "                                                 model=self.ema.ema.module if hasattr(self.ema.ema,\n",
    "                                                                                 'module') else self.ema.ema,\n",
    "                                                 single_cls=self.opt.single_cls,\n",
    "                                                 dataloader=self.testloader,\n",
    "                                                 save_dir=self.log_dir)\n",
    "\n",
    "                # Write\n",
    "                with open(self.results_file, 'a') as f:\n",
    "                    f.write(\n",
    "                        self.s + '%10.4g' * 7 % self.results + '\\n')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n",
    "                if len(self.opt.name) and self.opt.bucket:\n",
    "                    os.system(\n",
    "                        'gsutil cp %s gs://%s/results/results%s.txt' % (self.results_file, self.opt.bucket,\n",
    "                                                                        self.opt.name))\n",
    "\n",
    "                # Tensorboard\n",
    "                if self.tb_writer:\n",
    "                    tags = ['train/giou_loss', 'train/obj_loss', 'train/cls_loss',\n",
    "                            'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5',\n",
    "                            'metrics/mAP_0.5:0.95',\n",
    "                            'val/giou_loss', 'val/obj_loss', 'val/cls_loss']\n",
    "                    for x, tag in zip(list(self.mloss[:-1]) + list(self.results), tags):\n",
    "                        self.tb_writer.add_scalar(tag, x, epoch)\n",
    "\n",
    "                # Update best mAP\n",
    "                fi = fitness(\n",
    "                    np.array(self.results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n",
    "                if fi > self.best_fitness:\n",
    "                    self.best_fitness = fi\n",
    "\n",
    "            # Save model\n",
    "            save = (not self.opt.nosave) or (final_epoch and not self.opt.evolve)\n",
    "            if save:\n",
    "                with open(self.results_file, 'r') as f:  # create checkpoint\n",
    "                    ckpt = {'epoch': epoch,\n",
    "                            'best_fitness': self.best_fitness,\n",
    "                            'training_results': f.read(),\n",
    "                            'model': self.ema.ema.module if hasattr(self.ema, 'module') else self.ema.ema,\n",
    "                            'optimizer': None if final_epoch else self.optimizer.state_dict()}\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(ckpt, self.last)\n",
    "                if (self.best_fitness == fi) and not final_epoch:\n",
    "                    torch.save(ckpt, self.best)\n",
    "                del ckpt\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n",
    "    # end training\n",
    "\n",
    "    if rank in [-1, 0]:\n",
    "        # Strip optimizers\n",
    "        n = ('_' if len(self.opt.name) and not self.opt.name.isnumeric() else '') + self.opt.name\n",
    "        fresults, flast, fbest = 'results%s.txt' % n, self.wdir + 'last%s.pt' % n, self.wdir + 'best%s.pt' % n\n",
    "        for f1, f2 in zip([self.wdir + 'last.pt', self.wdir + 'best.pt', 'results.txt'], [flast, fbest, fresults]):\n",
    "            if os.path.exists(f1):\n",
    "                os.rename(f1, f2)  # rename\n",
    "                ispt = f2.endswith('.pt')  # is *.pt\n",
    "                strip_optimizer(f2) if ispt else None  # strip optimizer\n",
    "                os.system('gsutil cp %s gs://%s/weights' % (\n",
    "                    f2, self.opt.bucket)) if self.opt.bucket and ispt else None  # upload\n",
    "        # Finish\n",
    "        if not self.opt.evolve:\n",
    "            plot_results(save_dir=self.log_dir)  # save as results.png\n",
    "        print('%g epochs completed in %.3f hours.\\n' % (epoch - self.start_epoch + 1, (time.time() - t0) / 3600))\n",
    "\n",
    "    dist.destroy_process_group() if rank not in [-1, 0] else None\n",
    "    torch.cuda.empty_cache()\n",
    "    return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in fine_detector.dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = enumerate(coarse_detector.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (imgs, targets, paths, _) in pbar:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ZeroDivisionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0a6ed58e5853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 raise ValueError(\"images is expected to be a list of 3d tensors \"\n\u001b[1;32m     43\u001b[0m                                  \"of shape [C, H, W], got {}\".format(image.shape))\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtorch_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex-0.1-py3.7.egg/apex/amp/wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HalfTensor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FloatTensor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             new_args = utils.casted_args(cast_fn,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex-0.1-py3.7.egg/apex/amp/wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HalfTensor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FloatTensor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             new_args = utils.casted_args(cast_fn,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ZeroDivisionError"
     ]
    }
   ],
   "source": [
    "model.forward(imgs, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) (tensor(0.35927), tensor(0.22220), tensor(0.07003), tensor(0.07403))\n",
      "tensor(3.) (tensor(0.56303), tensor(0.43624), tensor(0.03055), tensor(0.03055))\n",
      "tensor(3.) (tensor(0.19139), tensor(0.35260), tensor(0.04946), tensor(0.06691))\n",
      "tensor(3.) (tensor(0.52739), tensor(0.78025), tensor(0.09891), tensor(0.02618))\n",
      "tensor(4.) (tensor(0.89983), tensor(0.20113), tensor(0.02814), tensor(0.02814))\n",
      "tensor(4.) (tensor(0.79798), tensor(0.17499), tensor(0.02278), tensor(0.04557))\n",
      "tensor(4.) (tensor(0.51854), tensor(0.06309), tensor(0.08309), tensor(0.02814))\n",
      "tensor(4.) (tensor(0.19890), tensor(0.69969), tensor(0.03619), tensor(0.03887))\n",
      "tensor(5.) (tensor(0.85896), tensor(0.04983), tensor(0.15888), tensor(0.06179))\n",
      "tensor(7.) (tensor(0.15063), tensor(0.75100), tensor(0.04629), tensor(0.02149))\n",
      "tensor(7.) (tensor(0.01673), tensor(0.40880), tensor(0.02645), tensor(0.07770))\n",
      "tensor(7.) (tensor(0.02665), tensor(0.29225), tensor(0.03968), tensor(0.07935))\n",
      "tensor(7.) (tensor(0.31925), tensor(0.37408), tensor(0.10911), tensor(0.03141))\n",
      "tensor(8.) (tensor(0.35522), tensor(0.09013), tensor(0.05448), tensor(0.06961))\n",
      "tensor(9.) (tensor(0.07590), tensor(0.74044), tensor(0.15180), tensor(0.12167))\n",
      "tensor(10.) (tensor(0.14425), tensor(0.79364), tensor(0.14748), tensor(0.07773))\n",
      "tensor(10.) (tensor(0.89761), tensor(0.68701), tensor(0.16741), tensor(0.07175))\n",
      "tensor(11.) (tensor(0.70888), tensor(0.49053), tensor(0.16021), tensor(0.06719))\n",
      "tensor(12.) (tensor(0.77974), tensor(0.12226), tensor(0.07840), tensor(0.04989))\n",
      "tensor(12.) (tensor(0.88843), tensor(0.93353), tensor(0.10334), tensor(0.13295))\n",
      "tensor(13.) (tensor(0.08486), tensor(0.40340), tensor(0.13472), tensor(0.17545))\n",
      "tensor(13.) (tensor(0.85561), tensor(0.39556), tensor(0.09713), tensor(0.04700))\n",
      "tensor(13.) (tensor(0.49687), tensor(0.73551), tensor(0.06893), tensor(0.04386))\n",
      "tensor(13.) (tensor(0.74125), tensor(0.56945), tensor(0.10653), tensor(0.05013))\n",
      "tensor(13.) (tensor(0.56893), tensor(0.94542), tensor(0.03133), tensor(0.03133))\n",
      "tensor(13.) (tensor(0.84308), tensor(0.59765), tensor(0.08459), tensor(0.03760))\n",
      "tensor(13.) (tensor(0.61436), tensor(0.85770), tensor(0.04073), tensor(0.03133))\n",
      "tensor(13.) (tensor(0.94177), tensor(0.59138), tensor(0.03760), tensor(0.03133))\n",
      "tensor(13.) (tensor(0.67859), tensor(0.89686), tensor(0.02506), tensor(0.02820))\n",
      "tensor(13.) (tensor(0.45457), tensor(0.18094), tensor(0.05953), tensor(0.05013))\n",
      "tensor(13.) (tensor(0.46867), tensor(0.27807), tensor(0.08773), tensor(0.02506))\n",
      "tensor(13.) (tensor(0.44204), tensor(0.43943), tensor(0.03446), tensor(0.02820))\n",
      "tensor(13.) (tensor(0.68642), tensor(0.45823), tensor(0.04700), tensor(0.02820))\n",
      "tensor(13.) (tensor(0.49843), tensor(0.49739), tensor(0.09086), tensor(0.03133))\n",
      "tensor(13.) (tensor(0.55013), tensor(0.53969), tensor(0.10653), tensor(0.02820))\n",
      "tensor(13.) (tensor(0.65039), tensor(0.33290), tensor(0.09399), tensor(0.02820))\n",
      "tensor(13.) (tensor(0.44987), tensor(0.66658), tensor(0.05013), tensor(0.03760))\n",
      "tensor(13.) (tensor(0.83524), tensor(0.17468), tensor(0.07519), tensor(0.03133))\n",
      "tensor(13.) (tensor(0.57363), tensor(0.64151), tensor(0.09713), tensor(0.03133))\n",
      "tensor(13.) (tensor(0.67545), tensor(0.66501), tensor(0.06893), tensor(0.03446))\n",
      "tensor(13.) (tensor(0.56423), tensor(0.75274), tensor(0.07206), tensor(0.02820))\n",
      "tensor(13.) (tensor(0.83054), tensor(0.72767), tensor(0.05326), tensor(0.02820))\n",
      "tensor(14.) (tensor(0.18952), tensor(0.57474), tensor(0.15778), tensor(0.09135))\n",
      "tensor(15.) (tensor(0.49786), tensor(0.29478), tensor(0.05144), tensor(0.03858))\n",
      "tensor(16.) (tensor(0.36035), tensor(0.18327), tensor(0.13723), tensor(0.04158))\n",
      "tensor(16.) (tensor(0.69302), tensor(0.40990), tensor(0.13723), tensor(0.04158))\n",
      "tensor(16.) (tensor(0.56515), tensor(0.91930), tensor(0.10188), tensor(0.03327))\n",
      "tensor(17.) (tensor(0.14012), tensor(0.28669), tensor(0.04407), tensor(0.15519))\n",
      "tensor(17.) (tensor(0.05870), tensor(0.28861), tensor(0.06897), tensor(0.15519))\n",
      "tensor(17.) (tensor(0.23688), tensor(0.54151), tensor(0.06897), tensor(0.11304))\n",
      "tensor(17.) (tensor(0.18802), tensor(0.29052), tensor(0.07089), tensor(0.15136))\n",
      "tensor(17.) (tensor(0.32310), tensor(0.90841), tensor(0.16477), tensor(0.07281))\n",
      "tensor(19.) (tensor(0.92207), tensor(0.49127), tensor(0.05804), tensor(0.08493))\n",
      "tensor(19.) (tensor(0.13007), tensor(0.89683), tensor(0.07078), tensor(0.02123))\n",
      "tensor(19.) (tensor(0.30347), tensor(0.48136), tensor(0.15430), tensor(0.05379))\n",
      "tensor(20.) (tensor(0.22225), tensor(0.04899), tensor(0.12493), tensor(0.04714))\n",
      "tensor(20.) (tensor(0.69132), tensor(0.82568), tensor(0.12964), tensor(0.08722))\n",
      "tensor(20.) (tensor(0.90347), tensor(0.97707), tensor(0.15793), tensor(0.04585))\n",
      "tensor(21.) (tensor(0.42204), tensor(0.76743), tensor(0.84407), tensor(0.46513))\n",
      "tensor(22.) (tensor(0.66694), tensor(0.05853), tensor(0.11409), tensor(0.11705))\n",
      "tensor(23.) (tensor(0.13539), tensor(0.79286), tensor(0.07759), tensor(0.10041))\n",
      "tensor(24.) (tensor(0.02106), tensor(0.82028), tensor(0.04212), tensor(0.08216))\n",
      "tensor(26.) (tensor(0.97793), tensor(0.45181), tensor(0.03098), tensor(0.15147))\n",
      "tensor(27.) (tensor(0.16192), tensor(0.39860), tensor(0.05873), tensor(0.02409))\n",
      "tensor(27.) (tensor(0.73266), tensor(0.02160), tensor(0.07078), tensor(0.04321))\n",
      "tensor(27.) (tensor(0.52108), tensor(0.05149), tensor(0.08132), tensor(0.06475))\n",
      "tensor(27.) (tensor(0.79892), tensor(0.32707), tensor(0.03464), tensor(0.05873))\n",
      "tensor(27.) (tensor(0.17322), tensor(0.90911), tensor(0.04217), tensor(0.10240))\n",
      "tensor(28.) (tensor(0.16071), tensor(0.53687), tensor(0.02247), tensor(0.07604))\n",
      "tensor(29.) (tensor(0.94897), tensor(0.19151), tensor(0.10205), tensor(0.04473))\n",
      "tensor(29.) (tensor(0.06536), tensor(0.68177), tensor(0.13072), tensor(0.04129))\n",
      "tensor(32.) (tensor(0.51963), tensor(0.26104), tensor(0.06330), tensor(0.06771))\n",
      "tensor(32.) (tensor(0.67787), tensor(0.50245), tensor(0.07360), tensor(0.06182))\n",
      "tensor(32.) (tensor(0.39009), tensor(0.82851), tensor(0.08391), tensor(0.06330))\n",
      "tensor(33.) (tensor(0.36047), tensor(0.05198), tensor(0.07658), tensor(0.04024))\n",
      "tensor(33.) (tensor(0.79075), tensor(0.42385), tensor(0.02336), tensor(0.06490))\n",
      "tensor(33.) (tensor(0.52207), tensor(0.82882), tensor(0.04413), tensor(0.05192))\n",
      "tensor(34.) (tensor(0.27855), tensor(0.22947), tensor(0.13241), tensor(0.05251))\n",
      "tensor(35.) (tensor(0.21311), tensor(0.23346), tensor(0.02818), tensor(0.03992))\n",
      "tensor(35.) (tensor(0.16262), tensor(0.38023), tensor(0.03992), tensor(0.02818))\n",
      "tensor(37.) (tensor(0.75344), tensor(0.93440), tensor(0.07106), tensor(0.09948))\n",
      "tensor(37.) (tensor(0.35409), tensor(0.29061), tensor(0.15917), tensor(0.05116))\n",
      "tensor(38.) (tensor(0.79320), tensor(0.48801), tensor(0.05532), tensor(0.04011))\n",
      "tensor(38.) (tensor(0.46473), tensor(0.90914), tensor(0.05117), tensor(0.06915))\n",
      "tensor(39.) (tensor(0.36314), tensor(0.57513), tensor(0.10832), tensor(0.09784))\n",
      "tensor(41.) (tensor(0.24278), tensor(0.22201), tensor(0.12222), tensor(0.14139))\n",
      "tensor(41.) (tensor(0.02493), tensor(0.45926), tensor(0.04987), tensor(0.05033))\n",
      "tensor(42.) (tensor(0.74489), tensor(0.38658), tensor(0.10218), tensor(0.08992))\n",
      "tensor(42.) (tensor(0.97054), tensor(0.70027), tensor(0.05892), tensor(0.03474))\n",
      "tensor(42.) (tensor(0.40974), tensor(0.56948), tensor(0.04496), tensor(0.03883))\n",
      "tensor(42.) (tensor(0.47616), tensor(0.85661), tensor(0.02657), tensor(0.02861))\n",
      "tensor(42.) (tensor(0.09400), tensor(0.84128), tensor(0.04700), tensor(0.03883))\n",
      "tensor(42.) (tensor(0.45368), tensor(0.47445), tensor(0.03474), tensor(0.03679))\n",
      "tensor(42.) (tensor(0.32697), tensor(0.50307), tensor(0.03065), tensor(0.03270))\n",
      "tensor(42.) (tensor(0.40565), tensor(0.43665), tensor(0.02452), tensor(0.03065))\n",
      "tensor(42.) (tensor(0.40361), tensor(0.49387), tensor(0.02452), tensor(0.03065))\n",
      "tensor(42.) (tensor(0.15838), tensor(0.56948), tensor(0.02452), tensor(0.02248))\n",
      "tensor(47.) (tensor(0.07540), tensor(0.62916), tensor(0.04786), tensor(0.10861))\n",
      "tensor(47.) (tensor(0.13338), tensor(0.89055), tensor(0.03866), tensor(0.09756))\n"
     ]
    }
   ],
   "source": [
    "for img_id, cls_num, bx, by, w, h in targets:\n",
    "    print(img_id, (bx, by, w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
